{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bead445d-ba6b-4181-b8ae-3e01b0e73cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gbnet.models import ordinal_regression\n",
    "\n",
    "# Data From\n",
    "# https://github.com/gagolews/ordinal-regression-data\n",
    "\n",
    "urls = [\n",
    "    f\"https://raw.githubusercontent.com/gagolews/ordinal-regression-data/refs/heads/master/{i}\"\n",
    "    for i in [\n",
    "        'abalone.csv',\n",
    "        'abalone_ord.csv',\n",
    "        'affairs.csv',\n",
    "        'ailerons.csv',\n",
    "        'auto_ord.csv',\n",
    "        'auto_riskness.csv',\n",
    "        'bostonhousing.csv',\n",
    "        'bostonhousing_ord.csv',\n",
    "        'californiahousing.csv',\n",
    "        'cement_strength.csv',\n",
    "        'fireman_example.csv',\n",
    "        'glass.csv',\n",
    "        'kinematics.csv',\n",
    "        'machine_ord.csv',\n",
    "        'skill.csv',\n",
    "        'stock_ord.csv',\n",
    "        'winequality-red.csv',\n",
    "        'winequality-white.csv',\n",
    "        'wisconsin_breast_ord.csv'\n",
    "    ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96b35f6-7240-4f5d-ae76-071c0bcc3a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/gbnet/lib/python3.12/site-packages/torch/autograd/graph.py:825: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1729646995093/work/torch/csrc/autograd/engine.cpp:1206.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    }
   ],
   "source": [
    "from xgboost.sklearn import XGBClassifier, XGBRegressor\n",
    "from lightgbm.sklearn import LGBMClassifier, LGBMRegressor\n",
    "\n",
    "results = []\n",
    "for url in urls:\n",
    "    df = pd.read_csv(url)\n",
    "    train = df.sample(int(df.shape[0] * 0.7), random_state=10110)\n",
    "    test = df[~df.index.isin(train.index)]\n",
    "    \n",
    "    train = train.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "    \n",
    "    xcols = [col for col in df.columns if not (col == 'response')]\n",
    "    ycol = 'response'\n",
    "\n",
    "    num_classes = df[ycol].nunique()\n",
    "    num_training = train.shape[0]\n",
    "    num_test = test.shape[0]\n",
    "\n",
    "    ordboost = ordinal_regression.GBOrd(num_classes)\n",
    "    ordboost_lgbm = ordinal_regression.GBOrd(num_classes, module_type='LGBModule')\n",
    "    xgbclass = XGBClassifier(objective='multi:softmax')\n",
    "    xgbmae = XGBRegressor(objective='reg:absoluteerror')\n",
    "    xgbhuber = XGBRegressor(objective='reg:pseudohubererror')\n",
    "    lgbclass = LGBMClassifier(objective='multiclass')\n",
    "    lgbmae = LGBMRegressor(objective='mae')\n",
    "    lgbhuber = LGBMRegressor(objective='huber')\n",
    "    \n",
    "    miny = train[ycol].min()\n",
    "    ordboost.fit(train[xcols], train[ycol])\n",
    "    ordboost_lgbm.fit(train[xcols], train[ycol])\n",
    "    xgbclass.fit(train[xcols], train[ycol] - miny)\n",
    "    xgbmae.fit(train[xcols], train[ycol])\n",
    "    xgbhuber.fit(train[xcols], train[ycol])\n",
    "    lgbclass.fit(train[xcols], train[ycol] - miny)\n",
    "    lgbmae.fit(train[xcols], train[ycol])\n",
    "    lgbhuber.fit(train[xcols], train[ycol])\n",
    "    \n",
    "    test['pred'] = ordboost.predict(test[xcols], return_logits=False)\n",
    "    test['pred_lgbm'] = ordboost_lgbm.predict(test[xcols], return_logits=False)\n",
    "    test['pred_xgbclass'] = xgbclass.predict(test[xcols]) + miny\n",
    "    test['pred_xgbmae'] = xgbmae.predict(test[xcols])\n",
    "    test['pred_xgbhuber'] = xgbhuber.predict(test[xcols])\n",
    "    test['pred_lgbclass'] = lgbclass.predict(test[xcols]) + miny\n",
    "    test['pred_lgbmae'] = lgbmae.predict(test[xcols])\n",
    "    test['pred_lgbhuber'] = lgbhuber.predict(test[xcols])\n",
    "    \n",
    "    results.append({\n",
    "        'dataset': url.split('/')[-1],\n",
    "        'num_classes': num_classes,\n",
    "        'training_size': num_training,\n",
    "        'test_size': num_test,\n",
    "        'test_mae_ord': ((test['response'] - test['pred']).abs()).mean(),\n",
    "        'test_mae_lgbm': ((test['response'] - test['pred_lgbm']).abs()).mean(),\n",
    "        'test_mae_xgbclass': ((test['response'] - test['pred_xgbclass']).abs()).mean(),\n",
    "        'test_mae_xgbmae': ((test['response'] - test['pred_xgbmae']).abs()).mean(),\n",
    "        'test_mae_xgbhuber': ((test['response'] - test['pred_xgbhuber']).abs()).mean(),\n",
    "        'test_mae_lgbclass': ((test['response'] - test['pred_lgbclass']).abs()).mean(),\n",
    "        'test_mae_lgbmae': ((test['response'] - test['pred_lgbmae']).abs()).mean(),\n",
    "        'test_mae_lgbhuber': ((test['response'] - test['pred_lgbhuber']).abs()).mean(),\n",
    "        'test_err_ord': ((test['response'] - test['pred']).abs() > 0).mean(),\n",
    "        'test_err_lgbm': ((test['response'] - test['pred_lgbm']).abs() > 0).mean(),\n",
    "        'test_err_xgbclass': ((test['response'] - test['pred_xgbclass']).abs() > 0).mean(),\n",
    "        'test_err_xgbmae': ((test['response'] - test['pred_xgbmae'].round()).abs() > 0).mean(),\n",
    "        'test_err_xgbhuber': ((test['response'] - test['pred_xgbhuber'].round()).abs() > 0).mean(),\n",
    "        'test_err_lgbclass': ((test['response'] - test['pred_lgbclass']).abs() > 0).mean(),\n",
    "        'test_err_lgbmae': ((test['response'] - test['pred_lgbmae'].round()).abs() > 0).mean(),\n",
    "        'test_err_lgbhuber': ((test['response'] - test['pred_lgbhuber'].round()).abs() > 0).mean(),\n",
    "    })    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4333d774-4657-4005-a68e-cb2594ccab81",
   "metadata": {},
   "outputs": [],
   "source": [
    "full = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0075e17-b7ab-46ac-bd9e-fae06ef63e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calculate_pairwise_win_rates(df, mae_columns):\n",
    "    \"\"\"\n",
    "    Calculate pairwise win rates between MAE columns where lower values are better.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing MAE columns\n",
    "    mae_columns (list): List of column names containing MAE values\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Matrix of win rates where entry [i,j] represents how often model i beats model j\n",
    "    \"\"\"\n",
    "    n_models = len(mae_columns)\n",
    "    win_rates = np.zeros((n_models, n_models))\n",
    "    \n",
    "    for i, model1 in enumerate(mae_columns):\n",
    "        for j, model2 in enumerate(mae_columns):\n",
    "            if i != j:\n",
    "                # Count how often model1 has lower MAE than model2\n",
    "                wins = (df[model1] < df[model2]).sum()\n",
    "                total_comparisons = len(df)\n",
    "                win_rates[i,j] = wins / total_comparisons\n",
    "    \n",
    "    # Create DataFrame with nice labels\n",
    "    result = pd.DataFrame(\n",
    "        win_rates,\n",
    "        columns=[col.replace('test_mae_', '') for col in mae_columns],\n",
    "        index=[col.replace('test_mae_', '') for col in mae_columns]\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f894ae08-ffbf-4df9-80fa-7786ff293f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_pairwise_win_rates(\n",
    "    full, [\n",
    "        'test_mae_ord',\n",
    "        'test_mae_lgbm',\n",
    "        'test_mae_xgbclass',\n",
    "        'test_mae_xgbmae',\n",
    "        'test_mae_xgbhuber',\n",
    "        'test_mae_lgbclass',\n",
    "        'test_mae_lgbmae',\n",
    "        'test_mae_lgbhuber'\n",
    "    ]\n",
    ").T.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de87271-20de-4c56-bfd6-3c368b868cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_pairwise_win_rates(\n",
    "    full, [\n",
    "        'test_err_ord',\n",
    "        'test_err_lgbm',\n",
    "        'test_err_xgbclass',\n",
    "        'test_err_xgbmae',\n",
    "        'test_err_xgbhuber',\n",
    "        'test_err_lgbclass',\n",
    "        'test_err_lgbmae',\n",
    "        'test_err_lgbhuber'\n",
    "    ]\n",
    ").T.sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5513cd51-909c-4b9b-b1ee-26f8cb3b3ed7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
